{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtaya++4oCX3V0a5XcaxZk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# Retrieve API key from Colab user data\n",
        "api_key = userdata.get('Open_API_Key')\n",
        "if not api_key:\n",
        "    raise ValueError(\"Open_API_Key not found in user data. Please set it in Colab.\")\n",
        "\n",
        "# Initialize OpenAI client with API key\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "gB1IrAm-s2Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython.display import display, Image  # display functions\n",
        "\n",
        "# Use an existing assistant\n",
        "assistant_id = \"asst_f48G6l0owhKzgjGXV7O32EZ2\"  # assistant's ID from playground\n",
        "\n",
        "# Create a new thread for interaction\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# Ensure a directory exists to save images\n",
        "image_dir = \"assistant_images\"\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "# Start the conversation loop\n",
        "while True:\n",
        "    # Step 1: Get user input\n",
        "    user_input = input(\"User: \")  # Prompt for next input\n",
        "\n",
        "    # Step 2: If the user types 'exit', break the loop and end the conversation\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Ending the session. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Step 3: Send the user input message to the assistant\n",
        "    client.beta.threads.messages.create(\n",
        "        thread_id=thread.id,\n",
        "        role=\"user\",\n",
        "        content=user_input\n",
        "    )\n",
        "\n",
        "    # Step 4: Ask the assistant to process the task\n",
        "    run = client.beta.threads.runs.create_and_poll(\n",
        "        thread_id=thread.id,\n",
        "        assistant_id=assistant_id,\n",
        "        instructions=\"Analyze the data and respond based on user request.\"\n",
        "    )\n",
        "\n",
        "    # Step 5: Fetch only the first assistant response\n",
        "    if run.status == 'completed':\n",
        "        messages = client.beta.threads.messages.list(thread_id=thread.id, order=\"desc\", limit=5)\n",
        "\n",
        "        # Find and print the first assistant response only\n",
        "        for msg in messages:\n",
        "            if msg.role == 'assistant':\n",
        "                first_content = msg.content[0]  # Get the first content block\n",
        "\n",
        "                if hasattr(first_content, \"text\"):\n",
        "                    # Handle text responses\n",
        "                    print(f\"Assistant: {first_content.text.value}\")\n",
        "\n",
        "                elif hasattr(first_content, \"image_file\"):\n",
        "                    # Handle image responses\n",
        "                    file_id = first_content.image_file.file_id\n",
        "                    response = client.files.content(file_id)  # Get binary image content\n",
        "\n",
        "                    # Save the image locally\n",
        "                    image_path = os.path.join(image_dir, f\"{file_id}.png\")\n",
        "                    with open(image_path, \"wb\") as f:\n",
        "                        f.write(response.content)\n",
        "\n",
        "                    print(f\"Assistant sent an image:\")\n",
        "                    # Resize image by setting width (adjust value as needed)\n",
        "                    display(Image(image_path, width=500))  # Width set to 500px for better readability\n",
        "\n",
        "                else:\n",
        "                    print(\"Assistant sent an unsupported response format.\")\n",
        "\n",
        "                break  # Stop after handling the first assistant response\n",
        "\n",
        "    else:\n",
        "        print(\"Assistant is processing your request. Please wait...\")\n",
        "\n",
        "    print(\"\\n\")  # Ensures a clean space before the next user prompt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNwg-Y3jtAnd",
        "outputId": "1ac51ad9-c5df-4b58-8cfc-56b3760a6230"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: how many accounts are targets?\n",
            "Assistant: There are 311 accounts marked as targets in the dataset.\n",
            "\n",
            "\n",
            "User: exit\n",
            "Ending the session. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updated Code:"
      ],
      "metadata": {
        "id": "-r12_aSTBJu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import ipywidgets as widgets\n",
        "import markdown\n",
        "from IPython.display import display, HTML, Image, Javascript\n",
        "\n",
        "# Use an existing assistant\n",
        "assistant_id = \"asst_f48G6l0owhKzgjGXV7O32EZ2\"  # Assistant's ID from playground\n",
        "\n",
        "# Create a new thread for interaction\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "def auto_focus_input():\n",
        "    \"\"\"Auto-focus input field after each response.\"\"\"\n",
        "    display(Javascript('document.getElementById(\"user_input\").focus();'))\n",
        "\n",
        "# Create input field and submit button\n",
        "user_input_widget = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type your message...',\n",
        "    description='User:',\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "submit_button = widgets.Button(description=\"Submit\")\n",
        "\n",
        "# Output area for conversation\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Function to handle user submission\n",
        "def handle_submit(_):\n",
        "    user_input = user_input_widget.value.strip()  # Remove extra spaces\n",
        "\n",
        "    if not user_input or len(user_input) < 2:  # Prevents accidental empty/single-letter inputs\n",
        "        return\n",
        "\n",
        "    # Clear input field\n",
        "    user_input_widget.value = \"\"\n",
        "\n",
        "    # Exit condition\n",
        "    if user_input.lower() == 'exit':\n",
        "        with output_area:\n",
        "            print(\"\\nEnding the session. Goodbye!\")\n",
        "        return\n",
        "\n",
        "    # Display User's message (with bold \"User:\" text)\n",
        "    with output_area:\n",
        "        display(HTML(f\"<b>User:</b> {markdown.markdown(user_input)}<br>\"))\n",
        "\n",
        "    # Send the user input to the assistant\n",
        "    client.beta.threads.messages.create(\n",
        "        thread_id=thread.id,\n",
        "        role=\"user\",\n",
        "        content=user_input\n",
        "    )\n",
        "\n",
        "    # Process the assistant's response\n",
        "    run = client.beta.threads.runs.create_and_poll(\n",
        "        thread_id=thread.id,\n",
        "        assistant_id=assistant_id,\n",
        "        instructions=\"Analyze the data and respond based on user request.\"\n",
        "    )\n",
        "\n",
        "    # Fetch assistant's response\n",
        "    if run.status == 'completed':\n",
        "        messages = client.beta.threads.messages.list(thread_id=thread.id, order=\"desc\", limit=5)\n",
        "\n",
        "        with output_area:\n",
        "            # Find the first assistant response\n",
        "            for msg in messages:\n",
        "                if msg.role == 'assistant':\n",
        "                    text_responses = []\n",
        "                    image_elements = []\n",
        "\n",
        "                    for content in msg.content:\n",
        "                        if hasattr(content, \"text\"):\n",
        "                            # Process assistant's text response as Markdown\n",
        "                            markdown_response = markdown.markdown(content.text.value.replace(\"\\n\", \"<br>\"))\n",
        "                            text_responses.append(markdown_response)\n",
        "\n",
        "                        elif hasattr(content, \"image_file\"):\n",
        "                            # Handle image responses\n",
        "                            file_id = content.image_file.file_id\n",
        "                            response = client.files.content(file_id)  # Get binary image content\n",
        "                            image_data = response.content\n",
        "\n",
        "                            # Convert image to base64 for displaying & download\n",
        "                            encoded_image = base64.b64encode(image_data).decode('utf-8')\n",
        "                            image_html = f'<img src=\"data:image/png;base64,{encoded_image}\" width=\"500\" style=\"max-width: 100%;\"/>'\n",
        "                            download_link = f'<a href=\"data:image/png;base64,{encoded_image}\" download=\"assistant_image.png\">Download Image</a>'\n",
        "                            image_elements.append(image_html + \"<br>\" + download_link)\n",
        "\n",
        "                    # Display assistant's text response (Markdown processed)\n",
        "                    if text_responses:\n",
        "                        wrapped_text = \"<br>\".join(text_responses)\n",
        "                        display(HTML(f'<div style=\"white-space: normal; word-wrap: break-word;\">'\n",
        "                                     f'<b>Assistant:</b> {wrapped_text}</div>'))\n",
        "\n",
        "                    # Display images with download links\n",
        "                    if image_elements:\n",
        "                        display(HTML(\"<br>\".join(image_elements)))\n",
        "\n",
        "                    break  # Stop after handling the first assistant response\n",
        "\n",
        "    else:\n",
        "        with output_area:\n",
        "            print(\"\\nAssistant is processing your request. Please wait...\")\n",
        "\n",
        "    # Auto-focus input for next interaction\n",
        "    auto_focus_input()\n",
        "\n",
        "# Attach submit button click event\n",
        "submit_button.on_click(handle_submit)\n",
        "\n",
        "# Add Enter key functionality to the input field\n",
        "user_input_widget.on_submit(handle_submit)\n",
        "\n",
        "# Display UI components\n",
        "display(output_area)\n",
        "display(widgets.HBox([user_input_widget, submit_button]))\n",
        "\n",
        "# Auto-focus input on load\n",
        "auto_focus_input()\n"
      ],
      "metadata": {
        "id": "t_ITxFGkBNB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}